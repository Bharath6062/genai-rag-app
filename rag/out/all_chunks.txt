As a Data Engineer II - Copilot Insights, you will participate in projects that improve availability, scalability, query performance, storage optimizations, and integration across multiple data systems. You will collaborate with partner teams, working across boundaries to create innovative solutions for analyzing and shaping data in a way that makes it easier for us to monitor, draw insights from, and run diagnostics on to see how the changes we make on a daily basis impact our customers and their interactions with Copilot in Office. Microsoft's mission is to empower every

our customers and their interactions with Copilot in Office. Microsoft's mission is to empower every person and every organization on the planet to achieve more. As employees we come together with a growth mindset, innovate to empower others, and collaborate to realize our shared goals. Each day we build on our values of respect, integrity, and accountability to create a culture of inclusion where everyone can thrive at work and beyond. Responsibilities Contributes in partnership with appropriate stakeholders (Project Managers, Technical Leads, Applied Scientists, Data Scientists) to determine user requirements

with appropriate stakeholders (Project Managers, Technical Leads, Applied Scientists, Data Scientists) to determine user requirements and needs for each workstream. Consider a variety of feedback channels to incorporate insights into future designs or solution fixes. Begins to incorporate appropriate continuous feedback loops measuring customer value, usage patterns, and other actionable metrics of value. Builds scalable and low maintenance data pipelines and dashboards to give the organization insights into Copilot usage and growth and enable them to break it down in meaningful ways to draw conclusions and gain insights. Develops, enhances,

to break it down in meaningful ways to draw conclusions and gain insights. Develops, enhances, and actively improves monitoring on top of the signals so that as there are significant movements us and our partners are automatically alerted to these changes. Then expanding the system to continuously make it easier to investigate and run diagnostics for signal movements and automating the root cause for movements. Learns and contributes to processes for the architecture of a product/solution feature and learns to create proposals by testing design hypotheses and helping to refine

product/solution feature and learns to create proposals by testing design hypotheses and helping to refine code plans under the technical leadership of others. Produces code to test hypotheses for technical solutions and assists with technical validation efforts. Helps with and participates in the development of design documents for simple designs or user stories with oversight, helps to determine the technology that will be leveraged, and how they will interact. Escalates findings from investigations to team members for design decisions. Learns about the implications of security and compliance requirements in systems

members for design decisions. Learns about the implications of security and compliance requirements in systems architecture. Learns to review work items to gain knowledge of product features in partnership with appropriate stakeholders. Assists and learns about breaking down work items into tasks and provides estimation. Escalates any issues that would cause a delay. Learns about, shares new ideas, and leverages data engineering tools to create, debug, and maintain robust pipelines. Uses internal tools and open source when possible. Embody our culture and values . Qualifications Required Qualifications: Master's Degree in

source when possible. Embody our culture and values . Qualifications Required Qualifications: Master's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field AND 1+ year(s) experience in business analytics, data science, software development, data modeling, or data engineering OR Bachelor's Degree in Computer Science, Math, Software Engineering, Computer Engineering, or related field AND 2+ years experience in business analytics, data science, software development, data modeling, or data engineering OR equivalent experience.

Bharath Reddy bharath@myjobemails.com | Minnesota, USA | +1 (763) 210-8511 | LinkedIn PROFESSIONAL SUMMARY Data Engineer with 3+ years of experience, have a proven track record in designing, implementing, and maintaining robust data infrastructure and pipelines. Proficient in Python, SQL, and Scala and Machine Learning, Specialize in ETL processes, leveraging tools like Apache Spark and Hadoop for distributed data processing and also proficient in NLP, Deep Learning, and machine learning models for analysis. TECHNICAL SKILLS  Programming & Libraries: Python, R, DAX, SQL, PL-SQL,NumPy, Pandas, Matplotlib, PySpark, Core Java, JavaScript

 Programming & Libraries: Python, R, DAX, SQL, PL-SQL,NumPy, Pandas, Matplotlib, PySpark, Core Java, JavaScript  Databases: MySQL, MS SQL Server, PostgreSQL, Snowflake, Oracle  Tools: Tableau, Power BI, SSRS, PowerApps, Tableau Prep, Google Data Flow, Hadoop, HDFS, Spark, Hive, Pig, MapReduce, EMR, Kafka, Sqoop  Data Science & Analytics: Machine Learning, Data Warehousing, Data Mining, Data bricks, Data Analytics, Data Modeling, Data Engineering, Database Querying, Microsoft Office 365, Power Query, DAX Functions, ETL Processing, Data Pipelines, Data Transformation, Ad-Hoc Analytics, Git Repository, JIRA, Data Verse, Data Visualization Docker, Jenkins,

Data Pipelines, Data Transformation, Ad-Hoc Analytics, Git Repository, JIRA, Data Verse, Data Visualization Docker, Jenkins, REST API's PROFESSIONAL EXPERIENCE Data Engineer, LTIMintree Aug 2023 – Present | Remote, USA  Developed a real-time ETL pipeline with Apache Kafka and Spark (Scala) to analyze customer behavior data on an e commerce platform, resulting in a 15% increase in sales conversions.  Utilized Hadoop and MapReduce to optimize large-scale historical data processing, implementing data mapping and transformation logic to ensure consistency and integrity across diverse sources.  Architected and optimized the Snowflake

logic to ensure consistency and integrity across diverse sources.  Architected and optimized the Snowflake data warehouse for efficient storage and retrieval of structured and unstructured data, ensuring scalability and performance for large datasets.  Automated comprehensive ETL workflows using Python and SQL for data extraction, transformation, and loading into the warehouse, while leveraging MS Excel for ad-hoc analysis and reporting.  Developed interactive dashboards using Power BI, enabling real-time visualization of key business metrics and empowering data-driven decision-making across departments.  Integrated predictive models using Azure Machine Learning Services

and empowering data-driven decision-making across departments.  Integrated predictive models using Azure Machine Learning Services for customer churn prediction and personalized product recommendations, deploying models and data pipelines in the cloud with Azure Data Factory, Azure Blob Storage, and Azure Synapse Analytics for secure, scalable infrastructure.  Automated CI/CD pipelines using Azure DevOps for efficient data infrastructure deployment, while optimizing Spark job performance and implementing monitoring tools for continuous improvement.  Achieved a 10% reduction in customer churn and a 15% increase in sales conversions by leveraging real-time insights and

in customer churn and a 15% increase in sales conversions by leveraging real-time insights and predictive analytics, while automating end-to-end data workflows to enhance operational efficiency. Data Engineer, HSBC Bank Mar 2021 – Dec 2022 | Hyderabad, India  Developed a highly scalable data integration and real-time analytics platform using Python and PySpark, streamlining data processing and improving the organization’s ability to derive actionable insights for better decision-making.  Architected and optimized ETL pipelines with Python, leveraging PySpark on Azure Databricks to enhance data processing by 40% and ensure high

Python, leveraging PySpark on Azure Databricks to enhance data processing by 40% and ensure high data quality across 10TB+ of structured, semi-structured, and unstructured data.  Utilized Pandas, NumPy, and SQLAlchemy for efficient data manipulation and transformation within the pipelines, improving data management workflows.  Integrated machine learning models with scikit-learn and TensorFlow into data pipelines, automating forecasting and customer behavior prediction, achieving a 30% increase in accuracy and saving over 100 man-hours per month.  Implemented CI/CD pipelines with Azure DevOps for continuous integration and deployment, reducing deployment time

 Implemented CI/CD pipelines with Azure DevOps for continuous integration and deployment, reducing deployment time by 50% and doubling the frequency of deployments to meet evolving business demands.  Managed Azure-based data warehouse infrastructure utilizing Azure Blob Storage, Azure Synapse Analytics, Azure Data Factory, and Azure Data Lake, ensuring uptime and enabling real-time data access for 50+ global stakeholders.  Leveraged Tableau for building interactive dashboards and visualizing key performance indicators, allowing real-time data analysis and better insights for stakeholders.  Conducted data management tasks, including data cleansing, validation, and

and better insights for stakeholders.  Conducted data management tasks, including data cleansing, validation, and enrichment, using Python and SQL to ensure accurate and up-to-date data for business analysis and reporting. EDUCATION Master of Science, Concordia University st Paul Jan 2022 - May 2024 | Minnesota, USA Information Technology and Management Bachelor of Technology, NRI Institute and Technology June 2018 – May 2022 | Vijayawada, India Electronics and Communication Engineering